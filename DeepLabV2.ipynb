{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/annanasnas/semantic_segmentation-25/blob/main/DeepLabV2.ipynb)"
      ],
      "metadata": {
        "id": "p-cEQRdrCgv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "ON08hQzIyGx8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPO = \"https://github.com/annanasnas/semantic_segmentation-25.git\"\n",
        "!git clone $REPO\n",
        "%cd /content/semantic_segmentation-25\n",
        "!pip install -q -r requirements.txt pyyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDdLtFA6yrE9",
        "outputId": "61ed500d-7f3d-4a51-ac96-cd14c5221470"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'semantic_segmentation-25'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 136 (delta 53), reused 110 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (136/136), 299.96 KiB | 1.26 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "/content/semantic_segmentation-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "usM7DI7Ny4Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"configs/deeplabv2.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "!python scripts/download_data.py\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = cfg[\"train\"][\"batch_size\"]\n",
        "epochs = cfg[\"train\"][\"epochs\"]\n",
        "data_dir = cfg[\"data\"][\"root\"]\n",
        "learning_rate = cfg[\"train\"][\"lr\"]\n",
        "img_size = cfg[\"data\"][\"img_size\"]\n",
        "name = cfg[\"model\"][\"name\"]"
      ],
      "metadata": {
        "id": "Eg08IBbau_S3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoaders"
      ],
      "metadata": {
        "id": "A-DW3ceDyygA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets.cityscapes import CityScapes\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])\n",
        "\n",
        "train_dataset = CityScapes(\n",
        "    root_dir=data_dir,\n",
        "    split=\"train\",\n",
        "    image_transform=image_transforms,\n",
        "    image_size=img_size\n",
        ")\n",
        "\n",
        "val_dataset = CityScapes(\n",
        "    root_dir=data_dir,\n",
        "    split=\"val\",\n",
        "    image_transform=image_transforms,\n",
        "    image_size=img_size\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "HG4PBKBLtG6S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "G_0-Q0bZy1Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "qICzC2Ovoz80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from models.deeplabv2.deeplabv2 import get_deeplab_v2\n",
        "from scripts.train import train_model\n",
        "from torch.amp import autocast, GradScaler\n",
        "from scripts.checkpoint import Checkpoint\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "model = get_deeplab_v2()\n",
        "optimizer = optim.SGD(model.optim_parameters(lr=learning_rate), momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "scheduler = optim.lr_scheduler.PolynomialLR(optimizer, total_iters=50, power=0.9)\n",
        "scaler = GradScaler()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "ckpt_dir = f\"/content/drive/MyDrive/semantic segmentation/checkpoints/{name}\"\n",
        "log_csv = f\"/content/drive/MyDrive/semantic segmentation/checkpoints/{name}/log.csv\"\n",
        "ckpt_mgr = Checkpoint(ckpt_dir)\n",
        "ckpt = Checkpoint(ckpt_dir)\n",
        "\n",
        "best_path = ckpt_dir / \"best.pth\"\n",
        "if best_path.exists():\n",
        "    ckpt = torch.load(best_path, map_location=\"cpu\", weights_only=False)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "    scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "    start_epoch = ckpt[\"epoch\"]\n",
        "    best_miou   = ckpt[\"best_miou\"]\n",
        "    df_prev = pd.read_csv(log_csv)\n",
        "    metrics = df_prev.to_dict(\"list\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    best_miou   = 0\n",
        "    metrics = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"val_miou\": []}\n",
        "\n",
        "train_model(model, train_dataloader, val_dataloader,\n",
        "            device, epochs, autocast, scaler,\n",
        "            optimizer, criterion, scheduler,\n",
        "            ckpt_mgr, start_epoch, best_miou,\n",
        "            log_csv, metrics)"
      ],
      "metadata": {
        "id": "meEY2Vcjvq1V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "90kG_qFLe3Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.utils import create_final_table, evaluate_miou\n",
        "\n",
        "model = get_deeplab_v2()\n",
        "best_model = torch.load(best_path, map_location=\"cpu\", weights_only=False)\n",
        "best_model.load_state_dict(best_model[\"model\"])\n",
        "\n",
        "df = create_final_table(model, name, device, (img_size, img_size*2), epochs)\n",
        "df[\"mIoU (%)\"] = evaluate_miou(best_model, val_dataloader, device) * 100\n",
        "print(df.to_markdown(index=False))"
      ],
      "metadata": {
        "id": "CEmozsX1eih7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEEfenZNkwSx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}